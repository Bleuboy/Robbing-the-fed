{"cells":[{"cell_type":"code","execution_count":1,"id":"a8ee9650","metadata":{"executionInfo":{"elapsed":10943,"status":"ok","timestamp":1719613304875,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"a8ee9650"},"outputs":[],"source":["import torch\n","import torchvision\n","from collections import namedtuple\n","import os\n","import matplotlib.pyplot as plt\n","from attacks.analytic_attack import ImprintAttacker\n","from modifications.imprint import ImprintBlock\n","from utils.breaching_utils import *\n","\n","import medmnist\n","from medmnist import INFO, Evaluator\n","\n","from opacus import PrivacyEngine\n","from opacus.validators import ModuleValidator\n","from torch.utils.data import DataLoader\n","import numpy as np\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":134,"id":"4a600fc7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6802,"status":"ok","timestamp":1719613311673,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"4a600fc7","outputId":"afc61d96-f93c-4f93-89f9-f92bbd733dbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: /Users/maximilianeckert/.medmnist/dermamnist_224.npz\n"]}],"source":["batch_size = 8 # Number of images in the user's batch. We have a small one here for visualization purposes\n","import random\n","random.seed(2324) # You can change this to get a new batch.\n","\n","transforms = torchvision.transforms.Compose(\n","    [\n","        torchvision.transforms.Resize(256),\n","        torchvision.transforms.CenterCrop(224),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=data_cfg_default.mean, std=data_cfg_default.std),\n","    ]\n",")\n","data_flag = 'dermamnist'\n","info = INFO[data_flag]\n","DataClass = getattr(medmnist, info['python_class'])\n","dataset = DataClass(split=\"val\", transform=transforms, download=True, size=224)\n","samples = [dataset[i] for i in random.sample(range(len(dataset)), batch_size)]\n","data_np = np.array([sample[0].numpy() for sample in samples])  # Convert list of numpy arrays to a single numpy array\n","data = torch.tensor(data_np)  # Convert the numpy array to a PyTorch tensor\n","#data = torch.stack([sample[0] for sample in samples])\n","labels = torch.tensor([sample[1] for sample in samples]).flatten()"]},{"cell_type":"markdown","id":"3d2c2795","metadata":{"id":"3d2c2795"},"source":["### Initialize your model"]},{"cell_type":"code","execution_count":135,"id":"6a6d6ba0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":494,"status":"ok","timestamp":1719613312160,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"6a6d6ba0","outputId":"5ef96f40-2bfd-4b7e-a1c8-c9a8eeb29e12"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["setup = dict(device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), dtype=torch.float)\n","\n","# This could be any model:\n","#model = torchvision.models.resnet18(weights = True)\n","# Modify the final layer to have 7 output classes\n","#model.fc = torch.nn.Linear(512, 7)\n","\n","model = torchvision.models.squeezenet1_0(pretrained=True)\n","\n","# Modify the final layer to have 7 output classes\n","model.classifier[1] = torch.nn.Conv2d(512, 7, kernel_size=(1, 1), stride=(1, 1))\n","\n","# Update the number of classes attribute\n","model.num_classes = 7\n","\n","loss_fn = torch.nn.CrossEntropyLoss()\n"]},{"cell_type":"code","execution_count":null,"id":"d8ac056c","metadata":{"executionInfo":{"elapsed":1232,"status":"ok","timestamp":1719613313390,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"d8ac056c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":136,"id":"189e7ef1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1719613313859,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"189e7ef1","outputId":"c7f7956b-fe91-4901-bb24-485e51c18acf"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1.6099, 1.4645, 0.8820, 1.9419, 1.6310, 0.2506, 0.4033],\n","        [2.2453, 1.2833, 1.3389, 1.8274, 2.6610, 0.7810, 1.1914],\n","        [1.4521, 0.9041, 0.9726, 1.2699, 1.3063, 0.3086, 0.6245],\n","        [3.5291, 2.9252, 1.9890, 1.3838, 2.2669, 0.5537, 0.8241],\n","        [2.9076, 2.3195, 1.4985, 1.9851, 1.8889, 0.2447, 1.0092],\n","        [2.5162, 2.1325, 1.7550, 1.5822, 3.7739, 1.0111, 1.3861],\n","        [1.7880, 0.5975, 0.7233, 2.1471, 2.0306, 0.5964, 1.0334],\n","        [1.8729, 1.0466, 1.1320, 2.3196, 2.5302, 0.4377, 0.6230]],\n","       grad_fn=<ReshapeAliasBackward0>)\n","tensor([5, 2, 5, 5, 4, 5, 5, 5])\n"]}],"source":["print(model(data))\n","print(labels)"]},{"cell_type":"code","execution_count":137,"id":"vYIhkXPgjW8_","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719613313859,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"vYIhkXPgjW8_"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":138,"id":"d9cc219c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: /Users/maximilianeckert/.medmnist/dermamnist_224.npz\n"]}],"source":["from torch.utils.data import Subset, DataLoader\n","\n","model = ModuleValidator.fix(model)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","training_set = DataClass(split=\"train\", transform=transforms, download=True, size=224)\n","subset_indices = np.arange(100)\n","subset_training_set = Subset(training_set, subset_indices)\n","data_loader = DataLoader(subset_training_set, batch_size=batch_size)"]},{"cell_type":"code","execution_count":139,"id":"cb343c53","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37316,"status":"ok","timestamp":1719613354825,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"cb343c53","outputId":"c91d059b-6273-45c5-842b-c3453c0bf2d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using sigma=1.1 and C=1.2\n"]},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n"]}],"source":["\n","# add opacus here -> problem with the model structure (ImprintBlock) so do it after the imprint block\n","#if hasattr(model, \"autograd_grad_sample_hooks\"):\n","#   del model.autograd_grad_sample_hooks\n","EPSILON = 50.0\n","EPOCHS = 2\n","DELTA = 1e-5\n","MAX_GRAD_NORM = 1.2\n","\n","privacy_engine = PrivacyEngine()\n","model, optimizer, data_loader = privacy_engine.make_private(\n","    module=model,\n","    optimizer=optimizer,\n","    data_loader=data_loader,\n","    max_grad_norm=MAX_GRAD_NORM,\n","    poisson_sampling= False,\n","    #grad_sample_mode= \"hooks\",\n","    noise_multiplier= 1.1,\n","    #grad_sample_mode=\"ew\",\n","    #epochs = 2,\n","    #target_epsilon = EPSILON,\n","    #target_delta = DELTA,\n",")\n","\n","print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")"]},{"cell_type":"code","execution_count":140,"id":"21af0a69","metadata":{},"outputs":[],"source":["# It will be modified maliciously:\n","input_dim = data_cfg_default.shape[0] * data_cfg_default.shape[1] * data_cfg_default.shape[2]\n","num_bins = 100 # Here we define number of imprint bins\n","block = ImprintBlock(input_dim, num_bins=num_bins)\n","model = torch.nn.Sequential(\n","    torch.nn.Flatten(), block, torch.nn.Unflatten(dim=1, unflattened_size=data_cfg_default.shape), model\n",")\n","secret = dict(weight_idx=0, bias_idx=1, shape=tuple(data_cfg_default.shape), structure=block.structure)\n","secrets = {\"ImprintBlock\": secret}"]},{"cell_type":"code","execution_count":141,"id":"fCabWbP1h64F","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1719613354826,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"fCabWbP1h64F"},"outputs":[],"source":["import torch\n","import numpy as np\n","from opacus.utils.batch_memory_manager import BatchMemoryManager\n","from tqdm.notebook import tqdm\n","\n","MAX_PHYSICAL_BATCH_SIZE = 8\n","DELTA = 1e-5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def accuracy(preds, labels):\n","    return (preds == labels).mean()\n","\n","def train(model, train_loader, optimizer, epoch, device):\n","    model.train()\n","    criterion = torch.nn.CrossEntropyLoss()\n","    #criterion.to(device)\n","    losses = []\n","    top1_acc = []\n","\n","    with BatchMemoryManager(\n","        data_loader=train_loader,\n","        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE,\n","        optimizer=optimizer\n","    ) as memory_safe_data_loader:\n","\n","        for i, (images, target) in enumerate(memory_safe_data_loader):\n","            optimizer.zero_grad()\n","            images = images.to(device)\n","            target = target.to(device)\n","\n","            # compute output\n","            output = model(images)\n","            target = target.flatten()\n","            loss = criterion(output, target)\n","\n","            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n","            labels = target.detach().cpu().numpy()\n","\n","            # measure accuracy and record loss\n","            acc = accuracy(preds, labels)\n","\n","            losses.append(loss.item())\n","            top1_acc.append(acc)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            if (i + 1) % 200 == 0:\n","                epsilon = privacy_engine.get_epsilon(DELTA)\n","                print(\n","                    f\"\\tTrain Epoch: {epoch} \\t\"\n","                    f\"Loss: {np.mean(losses):.6f} \"\n","                    f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n","                    f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n","                )"]},{"cell_type":"code","execution_count":95,"id":"ce8cdd5a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 11.13193203852727\n","Training finished\n"]}],"source":["# Model training\n","\n","for epoch in range(1):\n","    running_loss = 0.0\n","    for inputs, labels in data_loader:\n","        labels = labels.flatten()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(data_loader)}\")\n","\n","print(\"Training finished\")\n"]},{"cell_type":"code","execution_count":113,"id":"Ll2WBjlgi51_","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1719613354827,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"Ll2WBjlgi51_"},"outputs":[],"source":["def test(model, test_loader, device):\n","    model.eval()\n","    criterion = torch.nn.CrossEntropyLoss()\n","    #criterion.to(device)\n","    losses = []\n","    top1_acc = []\n","\n","    with torch.no_grad():\n","        for images, target in test_loader:\n","            images = images.to(device)\n","            target = target.to(device)\n","\n","            output = model(images)\n","            target = target.flatten()\n","            loss = criterion(output, target)\n","            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n","            labels = target.detach().cpu().numpy()\n","            acc = accuracy(preds, labels)\n","\n","            losses.append(loss.item())\n","            top1_acc.append(acc)\n","\n","    top1_avg = np.mean(top1_acc)\n","\n","    print(\n","        f\"\\tTest set:\"\n","        f\"Loss: {np.mean(losses):.6f} \"\n","        f\"Acc: {top1_avg * 100:.6f} \"\n","    )\n","    return np.mean(top1_acc)"]},{"cell_type":"code","execution_count":114,"id":"QKTcpvJyjC-C","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277,"referenced_widgets":["07b9c740d2844b56957d608ba8bd1658","30ef7a33b2534191b777486715eec4fc","ae24c854b7b848bcb1ab79815e982439","a5e10e7749d84af2bb02c20044c6e998","9d4fedb6bc1947e191e87cd6d9762812","685f472f6a1a42fbb67179da86eb6723","d7f787bf270b42eb8b58bdfdd213ce56","4a28240d800f4caead86e2622e0c932f","45619e3e7c4a4628a866d09b1a450d3f","cf25495e97c24d5cae2b22da577c625b","e6fbe665d6d04375a8b602e57b9ae134"]},"executionInfo":{"elapsed":157331,"status":"ok","timestamp":1719613512152,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"QKTcpvJyjC-C","outputId":"61a26dbc-ac82-4655-8a96-fcb40e6e88be"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8d852eefcdf44febb0d6e92b13cc1f6","version_major":2,"version_minor":0},"text/plain":["Epoch:   0%|          | 0/2 [00:00<?, ?epoch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tqdm.notebook import tqdm\n","#model.to(device)\n","\n","for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n","    train(model, data_loader, optimizer, epoch + 1, device)"]},{"cell_type":"code","execution_count":115,"id":"xvQGnjaGyWA-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4826,"status":"ok","timestamp":1719613516959,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"xvQGnjaGyWA-","outputId":"62747c9d-0ff9-40ac-cdaf-3d3fc524897b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: /Users/maximilianeckert/.medmnist/dermamnist_224.npz\n"]}],"source":["test_set = DataClass(split=\"test\", transform=transforms, download=True, size=224)\n","test_loader = DataLoader(test_set, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":116,"id":"oQaMJpBOyTsO","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13802,"status":"ok","timestamp":1719613530748,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"oQaMJpBOyTsO","outputId":"458ca6d5-d428-43bf-83aa-037a5ef3db99"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top1_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[113], line 13\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:149\u001b[0m, in \u001b[0;36mGradSampleModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torchvision/models/squeezenet.py:95\u001b[0m, in \u001b[0;36mSqueezeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 95\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["top1_acc = test(model, test_loader, device)"]},{"cell_type":"code","execution_count":162,"id":"be237393","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1719613530749,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"be237393"},"outputs":[{"ename":"NameError","evalue":"name '_modules' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[162], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_trained \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[43m_modules\u001b[49m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_trained\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n","\u001b[0;31mNameError\u001b[0m: name '_modules' is not defined"]}],"source":["\n","model_trained = model.__dict__[_modules]['3']\n","print(model_trained.__dict__.items())"]},{"cell_type":"code","execution_count":155,"id":"e4f7eeda","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1719613530749,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"e4f7eeda","outputId":"c8d8e5e4-db7a-40f8-9846-1499caec642b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"name":"stdout","output_type":"stream","text":["tensor([[1.4305, 1.0979, 0.6477, 1.6111, 1.3809, 0.1406, 0.1620],\n","        [1.6231, 0.8244, 0.7447, 1.5270, 2.3330, 0.3052, 0.7816],\n","        [1.1552, 0.5816, 0.6148, 0.9779, 1.0850, 0.1464, 0.3954],\n","        [3.0533, 2.3448, 1.6137, 0.8302, 1.9914, 0.3286, 0.2818],\n","        [2.8633, 1.9247, 1.0087, 1.5910, 1.5590, 0.0817, 0.5659],\n","        [1.8518, 1.5261, 1.3469, 0.8847, 3.3491, 0.4206, 0.6603],\n","        [1.3113, 0.1449, 0.4382, 2.0426, 1.5484, 0.3206, 0.5943],\n","        [1.3019, 0.5592, 0.4781, 2.1186, 2.0522, 0.2679, 0.3745]],\n","       grad_fn=<ReshapeAliasBackward0>)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[155], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#model_trained = model._module\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_trained(data))\n\u001b[0;32m----> 4\u001b[0m top1_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_trained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n","Cell \u001b[0;32mIn[113], line 13\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:149\u001b[0m, in \u001b[0;36mGradSampleModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torchvision/models/squeezenet.py:95\u001b[0m, in \u001b[0;36mSqueezeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 95\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#model_trained = model._module\n","\n","print(model_trained(data))\n","top1_acc = test(model_trained, test_loader, device)\n","print(labels)"]},{"cell_type":"markdown","id":"de491268","metadata":{"id":"de491268"},"source":["### Simulate an attacked FL protocol"]},{"cell_type":"code","execution_count":156,"id":"1f0f8bc5","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1719613530749,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"1f0f8bc5"},"outputs":[],"source":["\n","# This is the attacker:\n","attacker = ImprintAttacker(model_trained, loss_fn, attack_cfg_default, setup)\n","\n","#Server-side computation:\n","queries = [dict(parameters=[p for p in model_trained.parameters()], buffers=[b for b in model_trained.buffers()])]\n","server_payload = dict(queries=queries, data=data_cfg_default)\n","\n","#User-side computation:\n","\n","\n","loss = loss_fn(model_trained(data.to(device)), labels.to(device))\n","\n"]},{"cell_type":"code","execution_count":144,"id":"4e7d32ed","metadata":{},"outputs":[],"source":["def compute_gradients(model, loss):\n","    # Zero gradients\n","    for param in model.parameters():\n","        if param.grad is not None:\n","            param.grad.data.zero_()\n","    model.eval()\n","    # Perform backward pass manually\n","    loss.backward(retain_graph=True)\n","\n","    # Collect gradients\n","    gradients = []\n","    for param in model.parameters():\n","        gradients.append(param.grad.data.clone())\n","    \n","    return gradients"]},{"cell_type":"code","execution_count":151,"id":"5b9de37a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":875,"status":"ok","timestamp":1719613531608,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"5b9de37a","outputId":"89be9754-b26b-4976-9e45-9ff8d36db123"},"outputs":[],"source":["#print(len(shared_data[\"gradients\"]),len(shared_data[\"gradients\"][0]))\n","def manual_compute_gradients(model, loss):\n","    # Ensure gradients are zeroed\n","    model.eval()\n","    optimizer.zero_grad()\n","    \n","    # Perform backward pass to compute gradients\n","    loss.backward(retain_graph=True)\n","    \n","    # Collect gradients\n","    gradients = [param.grad.clone() for param in model.parameters() if param.grad is not None]\n","    \n","    return gradients\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6bb6fbbf","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":152,"id":"015fb821","metadata":{},"outputs":[{"ename":"IndexError","evalue":"pop from empty list","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[152], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mmanual_compute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_trained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[151], line 8\u001b[0m, in \u001b[0;36mmanual_compute_gradients\u001b[0;34m(model, loss)\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Perform backward pass to compute gradients\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Collect gradients\u001b[39;00m\n\u001b[1;32m     11\u001b[0m gradients \u001b[38;5;241m=\u001b[39m [param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:69\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:327\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    326\u001b[0m backprops \u001b[38;5;241m=\u001b[39m forward_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 327\u001b[0m activations, backprops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange_grad_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackprops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_reduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_reduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_functorch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS:\n\u001b[1;32m    334\u001b[0m     grad_sampler_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS[\u001b[38;5;28mtype\u001b[39m(module)]\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:397\u001b[0m, in \u001b[0;36mGradSampleModule.rearrange_grad_samples\u001b[0;34m(self, module, backprops, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_len\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# For packed sequences, max_batch_len is set in the forward of the model (e.g. the LSTM)\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# Otherwise we infer it here\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     module\u001b[38;5;241m.\u001b[39mmax_batch_len \u001b[38;5;241m=\u001b[39m _get_batch_size(\n\u001b[1;32m    394\u001b[0m         module\u001b[38;5;241m=\u001b[39mmodule,\n\u001b[1;32m    395\u001b[0m         batch_dim\u001b[38;5;241m=\u001b[39mbatch_dim,\n\u001b[1;32m    396\u001b[0m     )\n\u001b[0;32m--> 397\u001b[0m activations \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    399\u001b[0m n \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mmax_batch_len\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mIndexError\u001b[0m: pop from empty list"]}],"source":["gradients = manual_compute_gradients(model_trained, loss)"]},{"cell_type":"code","execution_count":157,"id":"c85eda67","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(2.8677, grad_fn=<NllLossBackward0>) <generator object Module.parameters at 0x978449540>\n"]}],"source":["print(loss,model_trained.parameters())"]},{"cell_type":"markdown","id":"2d3ffe67","metadata":{},"source":[]},{"cell_type":"code","execution_count":158,"id":"a46203d7","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1719613531608,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"a46203d7"},"outputs":[{"ename":"IndexError","evalue":"pop from empty list","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[158], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m shared_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     gradients\u001b[38;5;241m=\u001b[39m[\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_trained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m],\n\u001b[1;32m      3\u001b[0m     buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     num_data_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m      6\u001b[0m     local_hyperparams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:69\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:327\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    326\u001b[0m backprops \u001b[38;5;241m=\u001b[39m forward_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 327\u001b[0m activations, backprops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange_grad_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackprops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_reduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_reduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_functorch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS:\n\u001b[1;32m    334\u001b[0m     grad_sampler_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS[\u001b[38;5;28mtype\u001b[39m(module)]\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:397\u001b[0m, in \u001b[0;36mGradSampleModule.rearrange_grad_samples\u001b[0;34m(self, module, backprops, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_len\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# For packed sequences, max_batch_len is set in the forward of the model (e.g. the LSTM)\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# Otherwise we infer it here\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     module\u001b[38;5;241m.\u001b[39mmax_batch_len \u001b[38;5;241m=\u001b[39m _get_batch_size(\n\u001b[1;32m    394\u001b[0m         module\u001b[38;5;241m=\u001b[39mmodule,\n\u001b[1;32m    395\u001b[0m         batch_dim\u001b[38;5;241m=\u001b[39mbatch_dim,\n\u001b[1;32m    396\u001b[0m     )\n\u001b[0;32m--> 397\u001b[0m activations \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    399\u001b[0m n \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mmax_batch_len\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mIndexError\u001b[0m: pop from empty list"]}],"source":["\n","shared_data = dict(\n","    gradients=[torch.autograd.grad(loss, model_trained.parameters())],\n","    buffers=None,\n","    num_data_points=1,\n","    labels=labels,\n","    local_hyperparams=None,\n",")\n"]},{"cell_type":"markdown","id":"10d3f62a","metadata":{"id":"10d3f62a"},"source":["### Reconstruct data from the update"]},{"cell_type":"code","execution_count":148,"id":"91ade4a2","metadata":{"id":"91ade4a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([150528]) torch.Size([150528])\n","torch.Size([150528]) torch.Size([150528])\n"]},{"ename":"IndexError","evalue":"too many indices for tensor of dimension 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[148], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Attack:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m reconstructed_user_data, stats \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecrets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdryrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Documents/UNI/TU/3. SS24/DL in Medicine/robbing_the_fed-main/attacks/analytic_attack.py:83\u001b[0m, in \u001b[0;36mImprintAttacker.reconstruct\u001b[0;34m(self, server_payload, shared_data, server_secrets, dryrun)\u001b[0m\n\u001b[1;32m     80\u001b[0m         bias_grad[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m bias_grad[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     82\u001b[0m image_positions \u001b[38;5;241m=\u001b[39m bias_grad\u001b[38;5;241m.\u001b[39mnonzero()\n\u001b[0;32m---> 83\u001b[0m layer_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvert_fc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m server_secrets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImprintBlock\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     86\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m server_secrets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImprintBlock\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m](layer_inputs)\n","File \u001b[0;32m~/Documents/UNI/TU/3. SS24/DL in Medicine/robbing_the_fed-main/attacks/analytic_attack.py:49\u001b[0m, in \u001b[0;36mAnalyticAttacker.invert_fc_layer\u001b[0;34m(self, weight_grad, bias_grad, image_positions)\u001b[0m\n\u001b[1;32m     47\u001b[0m valid_classes \u001b[38;5;241m=\u001b[39m bias_grad \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_grad\u001b[38;5;241m.\u001b[39mshape, bias_grad\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 49\u001b[0m intermediates \u001b[38;5;241m=\u001b[39m \u001b[43mweight_grad\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m bias_grad[valid_classes, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(image_positions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     reconstruction_data \u001b[38;5;241m=\u001b[39m intermediates\n","\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"]}],"source":["# Attack:\n","reconstructed_user_data, stats = attacker.reconstruct(server_payload, shared_data, secrets, dryrun=False)"]},{"cell_type":"code","execution_count":null,"id":"6a910a92","metadata":{"id":"6a910a92"},"outputs":[],"source":["# Metrics?:\n","from utils.analysis import report\n","true_user_data = {'data': data, 'labels': labels}\n","metrics = report(reconstructed_user_data,\n","    true_user_data,\n","    server_payload,\n","    model, compute_ssim=False) # Can change to true and install a package...\n","print(f\"MSE: {metrics['mse']}, PSNR: {metrics['psnr']}, LPIPS: {metrics['lpips']}, SSIM: {metrics['ssim']} \")"]},{"cell_type":"markdown","id":"1777d351","metadata":{"id":"1777d351"},"source":["### Plot ground-truth data"]},{"cell_type":"code","execution_count":null,"id":"e0484998","metadata":{"id":"e0484998"},"outputs":[],"source":["plot_data(data_cfg_default, true_user_data, setup)\n","\n","# Create the \"images\" folder if it doesn't exist\n","if not os.path.exists(\"images\"):\n","    os.makedirs(\"images\")\n","\n","# Save the images inside the \"images\" folder\n","plt.savefig(\"images/true_user_data.png\")\n"]},{"cell_type":"markdown","id":"f410d7fd","metadata":{"id":"f410d7fd"},"source":["### Now plot reconstructed data"]},{"cell_type":"code","execution_count":null,"id":"2e7dd96c","metadata":{"id":"2e7dd96c"},"outputs":[],"source":["plot_data(data_cfg_default, reconstructed_user_data, setup)\n","# Save the images inside the \"images\" folder\n","plt.savefig(\"images/reconstructed_user_data.png\")"]},{"cell_type":"code","execution_count":null,"id":"0504255f","metadata":{"id":"0504255f"},"outputs":[],"source":["\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"07b9c740d2844b56957d608ba8bd1658":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30ef7a33b2534191b777486715eec4fc","IPY_MODEL_ae24c854b7b848bcb1ab79815e982439","IPY_MODEL_a5e10e7749d84af2bb02c20044c6e998"],"layout":"IPY_MODEL_9d4fedb6bc1947e191e87cd6d9762812"}},"30ef7a33b2534191b777486715eec4fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_685f472f6a1a42fbb67179da86eb6723","placeholder":"​","style":"IPY_MODEL_d7f787bf270b42eb8b58bdfdd213ce56","value":"Epoch: 100%"}},"45619e3e7c4a4628a866d09b1a450d3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a28240d800f4caead86e2622e0c932f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"685f472f6a1a42fbb67179da86eb6723":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d4fedb6bc1947e191e87cd6d9762812":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5e10e7749d84af2bb02c20044c6e998":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf25495e97c24d5cae2b22da577c625b","placeholder":"​","style":"IPY_MODEL_e6fbe665d6d04375a8b602e57b9ae134","value":" 2/2 [02:36&lt;00:00, 78.55s/epoch]"}},"ae24c854b7b848bcb1ab79815e982439":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a28240d800f4caead86e2622e0c932f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45619e3e7c4a4628a866d09b1a450d3f","value":2}},"cf25495e97c24d5cae2b22da577c625b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7f787bf270b42eb8b58bdfdd213ce56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6fbe665d6d04375a8b602e57b9ae134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}
