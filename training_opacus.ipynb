{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ee9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from attacks.analytic_attack import ImprintAttacker\n",
    "from modifications.imprint import ImprintBlock\n",
    "from utils.breaching_utils import *\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a600fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # Number of images in the user's batch. We have a small one here for visualization purposes\n",
    "import random\n",
    "random.seed(234324) # You can change this to get a new batch.\n",
    "\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=data_cfg_default.mean, std=data_cfg_default.std),\n",
    "    ]\n",
    ")\n",
    "data_flag = 'dermamnist'\n",
    "info = INFO[data_flag]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "dataset = DataClass(split=\"val\", transform=transforms, download=True, size=224)\n",
    "samples = [dataset[i] for i in random.sample(range(len(dataset)), batch_size)]\n",
    "data = torch.stack([sample[0] for sample in samples])\n",
    "labels = torch.tensor([sample[1] for sample in samples]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04579830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c2795",
   "metadata": {},
   "source": [
    "### Initialize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6d6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = dict(device=torch.device(\"cpu\"), dtype=torch.float)\n",
    "\n",
    "# This could be any model:\n",
    "model = torchvision.models.resnet18(pretrained = True)\n",
    "# Modify the final layer to have 7 output classes\n",
    "model.fc = torch.nn.Linear(512, 7)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ac056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will be modified maliciously:\n",
    "input_dim = data_cfg_default.shape[0] * data_cfg_default.shape[1] * data_cfg_default.shape[2]\n",
    "num_bins = 100 # Here we define number of imprint bins\n",
    "block = ImprintBlock(input_dim, num_bins=num_bins)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(), block, torch.nn.Unflatten(dim=1, unflattened_size=data_cfg_default.shape), model\n",
    ")\n",
    "secret = dict(weight_idx=0, bias_idx=1, shape=tuple(data_cfg_default.shape), structure=block.structure)\n",
    "secrets = {\"ImprintBlock\": secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189e7ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3597,  0.1574, -0.1533, -0.1983, -0.0081,  0.6455, -0.2488]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "print(model(data))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb343c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModuleValidator.fix(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  \n",
    "training_set = DataClass(split=\"train\", transform=transforms, download=True, size=224)\n",
    "data_loader = DataLoader(training_set, batch_size=batch_size)\n",
    "\n",
    "# add opacus here -> problem with the model structure (ImprintBlock) so do it after the imprint block\n",
    "if hasattr(model, \"autograd_grad_sample_hooks\"):\n",
    "    del model.autograd_grad_sample_hooks\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, data_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=data_loader, \n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1,\n",
    "    poisson_sampling= False,\n",
    "    grad_sample_mode= \"hooks\",\n",
    "    #grad_sample_mode=\"ew\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc0943cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5993, -0.2297,  0.0201, -0.1273, -0.7185,  0.9087,  0.2166]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "print(model(data))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d99bfac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Per sample gradient is not initialized. Not updated in backward pass?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(data_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:518\u001b[0m, in \u001b[0;36mDPOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    516\u001b[0m         closure()\n\u001b[0;32m--> 518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:496\u001b[0m, in \u001b[0;36mDPOptimizer.pre_step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03mPerform actions specific to ``DPOptimizer`` before calling\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03munderlying  ``optimizer.step()``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m        returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# The corner case when the optimizer has no trainable parameters.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Essentially the DPOptimizer act as a normal optimizer\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_samples\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_samples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_and_accumulate()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:345\u001b[0m, in \u001b[0;36mDPOptimizer.grad_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[0;32m--> 345\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_flat_grad_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:282\u001b[0m, in \u001b[0;36mDPOptimizer._get_flat_grad_sample\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer sample gradient not found. Are you using GradSampleModule?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad_sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer sample gradient is not initialized. Not updated in backward pass?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p\u001b[38;5;241m.\u001b[39mgrad_sample, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    286\u001b[0m     ret \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad_sample\n",
      "\u001b[0;31mValueError\u001b[0m: Per sample gradient is not initialized. Not updated in backward pass?"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in data_loader:\n",
    "        labels = labels.flatten()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(data_loader)}\")\n",
    "\n",
    "print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be237393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "safe_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = model\n",
    "\n",
    "print(model_trained(data))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de491268",
   "metadata": {},
   "source": [
    "### Simulate an attacked FL protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the attacker:\n",
    "attacker = ImprintAttacker(model_trained, loss_fn, attack_cfg_default, setup)\n",
    "\n",
    "#Server-side computation:\n",
    "queries = [dict(parameters=[p for p in model_trained.parameters()], buffers=[b for b in model_trained.buffers()])]\n",
    "server_payload = dict(queries=queries, data=data_cfg_default)\n",
    "\n",
    "#User-side computation:\n",
    "\n",
    "\n",
    "loss = loss_fn(model_trained(data), labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9de37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46203d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_data = dict(\n",
    "    gradients=[torch.autograd.grad(loss, model_trained.parameters())],\n",
    "    buffers=None,\n",
    "    num_data_points=1,\n",
    "    labels=labels,\n",
    "    local_hyperparams=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3f62a",
   "metadata": {},
   "source": [
    "### Reconstruct data from the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ade4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack:\n",
    "reconstructed_user_data, stats = attacker.reconstruct(server_payload, shared_data, secrets, dryrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a910a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics?: \n",
    "from utils.analysis import report\n",
    "true_user_data = {'data': data, 'labels': labels}\n",
    "metrics = report(reconstructed_user_data,\n",
    "    true_user_data,\n",
    "    server_payload,\n",
    "    model, compute_ssim=False) # Can change to true and install a package...\n",
    "print(f\"MSE: {metrics['mse']}, PSNR: {metrics['psnr']}, LPIPS: {metrics['lpips']}, SSIM: {metrics['ssim']} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777d351",
   "metadata": {},
   "source": [
    "### Plot ground-truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0484998",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data_cfg_default, true_user_data, setup)\n",
    "\n",
    "# Create the \"images\" folder if it doesn't exist\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")\n",
    "\n",
    "# Save the images inside the \"images\" folder\n",
    "plt.savefig(\"images/true_user_data.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410d7fd",
   "metadata": {},
   "source": [
    "### Now plot reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data_cfg_default, reconstructed_user_data, setup)\n",
    "# Save the images inside the \"images\" folder\n",
    "plt.savefig(\"images/reconstructed_user_data.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
