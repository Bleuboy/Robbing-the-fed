{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ee9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba321ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacks.analytic_attack import ImprintAttacker\n",
    "from modifications.imprint import ImprintBlock\n",
    "from utils.breaching_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac450e",
   "metadata": {},
   "source": [
    "# Attack begins here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c2795",
   "metadata": {},
   "source": [
    "### Initialize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6d6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = dict(device=torch.device(\"cpu\"), dtype=torch.float)\n",
    "\n",
    "# This could be any model:\n",
    "model = torchvision.models.resnet18()\n",
    "model.eval()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# It will be modified maliciously:\n",
    "input_dim = data_cfg_default.shape[0] * data_cfg_default.shape[1] * data_cfg_default.shape[2]\n",
    "num_bins = 100 # Here we define number of imprint bins\n",
    "block = ImprintBlock(input_dim, num_bins=num_bins)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(), block, torch.nn.Unflatten(dim=1, unflattened_size=data_cfg_default.shape), model\n",
    ")\n",
    "secret = dict(weight_idx=0, bias_idx=1, shape=tuple(data_cfg_default.shape), structure=block.structure)\n",
    "secrets = {\"ImprintBlock\": secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bb6e2",
   "metadata": {},
   "source": [
    "### And your dataset (ImageNet by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e94352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms = torchvision.transforms.Compose(\n",
    "#     [\n",
    "#         torchvision.transforms.Resize(256),\n",
    "#         torchvision.transforms.CenterCrop(224),\n",
    "#         torchvision.transforms.ToTensor(),\n",
    "#         torchvision.transforms.Normalize(mean=data_cfg_default.mean, std=data_cfg_default.std),\n",
    "#     ]\n",
    "# )\n",
    "# dataset = torchvision.datasets.ImageNet(root=\"~/data/imagenet/\", split=\"val\", transform=transforms)\n",
    "# batch_size = 64 # Number of images in the user's batch. We have a small one here for visualization purposes\n",
    "# import random\n",
    "# random.seed(123) # You can change this to get a new batch. \n",
    "# samples = [dataset[i] for i in random.sample(range(len(dataset)), batch_size)]\n",
    "# data = torch.stack([sample[0] for sample in samples])\n",
    "# labels = torch.tensor([sample[1] for sample in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6520f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/maximilianeckert/.medmnist/dermamnist_224.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/ywdtx99d1vl0ptsg1fy494_40000gn/T/ipykernel_1772/2130135357.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  labels = torch.tensor([sample[1] for sample in samples]).flatten()\n"
     ]
    }
   ],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "batch_size = 8 # Number of images in the user's batch. We have a small one here for visualization purposes\n",
    "import random\n",
    "random.seed(234324) # You can change this to get a new batch.\n",
    "\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=data_cfg_default.mean, std=data_cfg_default.std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "data_flag = 'dermamnist'\n",
    "info = INFO[data_flag]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "dataset = DataClass(split=\"val\", transform=transforms, download=True, size=224)\n",
    "samples = [dataset[i] for i in random.sample(range(len(dataset)), batch_size)]\n",
    "data = torch.stack([sample[0] for sample in samples])\n",
    "labels = torch.tensor([sample[1] for sample in samples]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df45f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 5, 5, 5, 0, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de491268",
   "metadata": {},
   "source": [
    "### Simulate an attacked FL protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ec154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327f20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/maximilianeckert/.medmnist/dermamnist_224.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = ModuleValidator.fix(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Fix: Added missing code for optimizer initialization\n",
    "training_set = DataClass(split=\"train\", transform=transforms, download=True, size=224)\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#if hasattr(model, \"autograd_grad_sample_hooks\"):\n",
    "#    del model.autograd_grad_sample_hooks\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, data_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=training_loader, \n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "# Run a forward pass to ensure activations are detected\n",
    "data, labels = next(iter(training_loader))\n",
    "\n",
    "# Ensure labels are 1D\n",
    "if labels.ndim > 1:\n",
    "    labels = labels.flatten()\n",
    "\n",
    "# Forward pass to ensure activations are detected\n",
    "_ = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f95014",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9943502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradSampleModule(Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): ImprintBlock(\n",
      "    (linear0): Linear(in_features=150528, out_features=100, bias=True)\n",
      "    (linear2): Linear(in_features=100, out_features=150528, bias=True)\n",
      "    (nonlin): ReLU()\n",
      "  )\n",
      "  (2): Unflatten(dim=1, unflattened_size=(3, 224, 224))\n",
      "  (3): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0f8bc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No activations detected for <class 'torch.nn.modules.linear.Linear'>, run forward after add_hooks(model)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# User-side computation:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(model(data), labels)\n\u001b[1;32m      9\u001b[0m shared_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m---> 10\u001b[0m     gradients\u001b[38;5;241m=\u001b[39m[\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m],\n\u001b[1;32m     11\u001b[0m     buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     num_data_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     13\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m     14\u001b[0m     local_hyperparams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:69\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:327\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    326\u001b[0m backprops \u001b[38;5;241m=\u001b[39m forward_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 327\u001b[0m activations, backprops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange_grad_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackprops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_reduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_reduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_functorch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS:\n\u001b[1;32m    334\u001b[0m     grad_sampler_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS[\u001b[38;5;28mtype\u001b[39m(module)]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/grad_sample/grad_sample_module.py:383\u001b[0m, in \u001b[0;36mGradSampleModule.rearrange_grad_samples\u001b[0;34m(self, module, backprops, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03mRearrange activations and grad_samples based on loss reduction and batch dim\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m    batch_first: True is batch dimension is first\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo activations detected for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(module)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m run forward after add_hooks(model)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m     )\n\u001b[1;32m    388\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m RNNLinear \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_len\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# For packed sequences, max_batch_len is set in the forward of the model (e.g. the LSTM)\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# Otherwise we infer it here\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: No activations detected for <class 'torch.nn.modules.linear.Linear'>, run forward after add_hooks(model)"
     ]
    }
   ],
   "source": [
    "\n",
    "# This is the attacker:\n",
    "attacker = ImprintAttacker(model, loss_fn, attack_cfg_default, setup)\n",
    "\n",
    "# Server-side computation:\n",
    "queries = [dict(parameters=[p for p in model.parameters()], buffers=[b for b in model.buffers()])]\n",
    "server_payload = dict(queries=queries, data=data_cfg_default)\n",
    "# User-side computation:\n",
    "loss = loss_fn(model(data), labels)\n",
    "shared_data = dict(\n",
    "    gradients=[torch.autograd.grad(loss, model.parameters())],\n",
    "    buffers=None,\n",
    "    num_data_points=1,\n",
    "    labels=labels,\n",
    "    local_hyperparams=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3f62a",
   "metadata": {},
   "source": [
    "### Reconstruct data from the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ade4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack:\n",
    "reconstructed_user_data, stats = attacker.reconstruct(server_payload, shared_data, secrets, dryrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a910a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics?: \n",
    "from utils.analysis import report\n",
    "true_user_data = {'data': data, 'labels': labels}\n",
    "metrics = report(reconstructed_user_data,\n",
    "    true_user_data,\n",
    "    server_payload,\n",
    "    model, compute_ssim=False) # Can change to true and install a package...\n",
    "print(f\"MSE: {metrics['mse']}, PSNR: {metrics['psnr']}, LPIPS: {metrics['lpips']}, SSIM: {metrics['ssim']} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777d351",
   "metadata": {},
   "source": [
    "### Plot ground-truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0484998",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data_cfg_default, true_user_data, setup)\n",
    "\n",
    "# Create the \"images\" folder if it doesn't exist\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")\n",
    "\n",
    "# Save the images inside the \"images\" folder\n",
    "plt.savefig(\"images/true_user_data.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410d7fd",
   "metadata": {},
   "source": [
    "### Now plot reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data_cfg_default, reconstructed_user_data, setup)\n",
    "# Save the images inside the \"images\" folder\n",
    "plt.savefig(\"images/reconstructed_user_data.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
