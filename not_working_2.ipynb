{"cells":[{"cell_type":"code","execution_count":1,"id":"a8ee9650","metadata":{"executionInfo":{"elapsed":5614,"status":"ok","timestamp":1719226442401,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"a8ee9650"},"outputs":[],"source":["import torch\n","import torchvision\n","from collections import namedtuple\n","import os\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","\n","from attacks.analytic_attack import ImprintAttacker\n","from modifications.imprint import ImprintBlock\n","from utils.breaching_utils import *\n","from opacus import GradSampleModule\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","id":"33ac450e","metadata":{"id":"33ac450e"},"source":["# Attack begins here:"]},{"cell_type":"markdown","id":"3d2c2795","metadata":{"id":"3d2c2795"},"source":["### Initialize your model"]},{"cell_type":"code","execution_count":2,"id":"6a6d6ba0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":769,"status":"ok","timestamp":1719226450299,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"6a6d6ba0","outputId":"d37277b6-e167-4db1-ad6c-af7729972012"},"outputs":[],"source":["setup = dict(device=torch.device(\"cpu\"), dtype=torch.float)\n","\n","# This could be any model:\n","model = torchvision.models.resnet18(num_classes=7)\n","\n","model.eval()\n","loss_fn = torch.nn.CrossEntropyLoss()\n","# It will be modified maliciously:\n","input_dim = data_cfg_default.shape[0] * data_cfg_default.shape[1] * data_cfg_default.shape[2]\n","num_bins = 100 # Here we define number of imprint bins\n","\n"]},{"cell_type":"markdown","id":"319bb6e2","metadata":{"id":"319bb6e2"},"source":["### And your dataset (ImageNet by default)"]},{"cell_type":"code","execution_count":3,"id":"e6520f8d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6078,"status":"ok","timestamp":1719226458762,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"e6520f8d","outputId":"89c713b1-0db7-4679-ca91-8c0bc97c1bb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: /Users/maximilianeckert/.medmnist/dermamnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/c4/ywdtx99d1vl0ptsg1fy494_40000gn/T/ipykernel_1497/2855271861.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n","  labels = torch.tensor([sample[1] for sample in samples]).flatten()\n"]}],"source":["import medmnist\n","from medmnist import INFO, Evaluator\n","\n","batch_size = 4 # Number of images in the user's batch. We have a small one here for visualization purposes\n","import random\n","random.seed(234324) # You can change this to get a new batch.\n","\n","transforms = torchvision.transforms.Compose(\n","    [\n","        torchvision.transforms.Resize(256),\n","        torchvision.transforms.CenterCrop(224),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=data_cfg_default.mean, std=data_cfg_default.std),\n","    ]\n",")\n","\n","\n","data_flag = 'dermamnist'\n","info = INFO[data_flag]\n","DataClass = getattr(medmnist, info['python_class'])\n","dataset = DataClass(split=\"val\", transform=transforms, download=True, size=224)\n","samples = [dataset[i] for i in random.sample(range(len(dataset)), batch_size)]\n","data = torch.stack([sample[0] for sample in samples])\n","labels = torch.tensor([sample[1] for sample in samples]).flatten()"]},{"cell_type":"code","execution_count":4,"id":"KV6YSwGSh3oi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1719226458763,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"KV6YSwGSh3oi","outputId":"771597a4-1784-4f0b-b30b-849b4e4fc39f"},"outputs":[],"source":["block = ImprintBlock(input_dim, num_bins=num_bins)\n","model = torch.nn.Sequential(\n","    torch.nn.Flatten(), block, torch.nn.Unflatten(dim=1, unflattened_size=data_cfg_default.shape), model\n",")\n","secret = dict(weight_idx=0, bias_idx=1, shape=tuple(data_cfg_default.shape), structure=block.structure)\n","secrets = {\"ImprintBlock\": secret}"]},{"cell_type":"code","execution_count":5,"id":"327f20cf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20689,"status":"ok","timestamp":1719226479841,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"327f20cf","outputId":"31bd3fb5-d1f9-4d37-825f-d69af6d18af9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: /Users/maximilianeckert/.medmnist/dermamnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n"]}],"source":["from opacus import PrivacyEngine\n","from opacus.validators import ModuleValidator\n","from torch.utils.data import DataLoader\n","\n","model = ModuleValidator.fix(model)\n","model = GradSampleModule(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Fix: Added missing code for optimizer initialization\n","training_set = DataClass(split=\"train\", transform=transforms, download=True, size=224)\n","training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n","\n","#if hasattr(model, \"autograd_grad_sample_hooks\"):\n","#    del model.autograd_grad_sample_hooks\n","\n","privacy_engine = PrivacyEngine()\n","model, optimizer, data_loader = privacy_engine.make_private(\n","    module=model,\n","    optimizer=optimizer,\n","    data_loader=training_loader,\n","    noise_multiplier=1.1,\n","    max_grad_norm=1.0,\n","    poisson_sampling=False,\n","    #grad_sample_mode=\"hooks\",\n","\n",")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"81428bdd","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"id":"bc9aa465","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.0280, -1.1038,  0.0524,  0.3591,  0.2203, -0.2732, -0.1617],\n","        [-1.0279, -1.1035,  0.0522,  0.3590,  0.2204, -0.2732, -0.1618],\n","        [-1.0276, -1.1025,  0.0516,  0.3589,  0.2206, -0.2731, -0.1621],\n","        [-1.0275, -1.1021,  0.0514,  0.3588,  0.2206, -0.2731, -0.1622]],\n","       grad_fn=<AddmmBackward0>)\n","tensor([5, 5, 5, 5])\n"]},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]}],"source":["print(model(data))\n","print(labels)"]},{"cell_type":"code","execution_count":7,"id":"u12JqhPWp2sX","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4067559,"status":"ok","timestamp":1719230547389,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"u12JqhPWp2sX","outputId":"ac461f27-9b27-46d8-b396-752872c7d79d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch 1/1752\n"]},{"ename":"ValueError","evalue":"Per sample gradient is not initialized. Not updated in backward pass?","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# train_classification(model, optimizer, training_loader, criterion, num_epochs=5)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model for classification\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mtrain_classification\u001b[0;34m(model, optimizer, data_loader, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mdataset)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:518\u001b[0m, in \u001b[0;36mDPOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    516\u001b[0m         closure()\n\u001b[0;32m--> 518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:496\u001b[0m, in \u001b[0;36mDPOptimizer.pre_step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03mPerform actions specific to ``DPOptimizer`` before calling\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03munderlying  ``optimizer.step()``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m        returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# The corner case when the optimizer has no trainable parameters.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Essentially the DPOptimizer act as a normal optimizer\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_samples\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_samples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_and_accumulate()\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:345\u001b[0m, in \u001b[0;36mDPOptimizer.grad_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams:\n\u001b[0;32m--> 345\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_flat_grad_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/opacus/optimizers/optimizer.py:282\u001b[0m, in \u001b[0;36mDPOptimizer._get_flat_grad_sample\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer sample gradient not found. Are you using GradSampleModule?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad_sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer sample gradient is not initialized. Not updated in backward pass?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p\u001b[38;5;241m.\u001b[39mgrad_sample, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    286\u001b[0m     ret \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad_sample\n","\u001b[0;31mValueError\u001b[0m: Per sample gradient is not initialized. Not updated in backward pass?"]}],"source":["criterion = nn.CrossEntropyLoss()\n","\n","# Training function for classification\n","def train_classification(model, optimizer, data_loader, criterion, num_epochs=5):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i, (inputs, labels) in enumerate(data_loader):\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","\n","            # Flatten labels if necessary (assuming labels shape is (batch_size, 1))\n","            labels = labels.squeeze()\n","\n","            # Print outputs and labels for debugging\n","            print(f\"Batch {i+1}/{len(data_loader)}\")\n","            # print(\"Outputs:\", outputs)\n","            # print(\"Labels:\", labels)\n","\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item() * inputs.size(0)\n","\n","        epoch_loss = running_loss / len(data_loader.dataset)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n","\n","# Example usage\n","# train_classification(model, optimizer, training_loader, criterion, num_epochs=5)\n","\n","# Train the model for classification\n","train_classification(model, optimizer, training_loader, criterion, num_epochs=1)"]},{"cell_type":"code","execution_count":null,"id":"8907db5c","metadata":{},"outputs":[],"source":["model_trained = model\n"]},{"cell_type":"code","execution_count":null,"id":"1f0f8bc5","metadata":{"executionInfo":{"elapsed":303,"status":"ok","timestamp":1719230566092,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"1f0f8bc5"},"outputs":[],"source":["# This is the attacker:\n","attacker = ImprintAttacker(model_trained, loss_fn, attack_cfg_default, setup)\n","\n","# Server-side computation:\n","queries = [dict(parameters=[p for p in model_trained.parameters()], buffers=[b for b in model_trained.buffers()])]\n","server_payload = dict(queries=queries, data=data_cfg_default)\n","# User-side computation:\n","loss = loss_fn(model_trained(data), labels)\n"]},{"cell_type":"code","execution_count":null,"id":"6vBykzYto7S8","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1719226265490,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"6vBykzYto7S8"},"outputs":[],"source":["shared_data = dict(\n","    gradients = [param.grad for param in model_trained.parameters() if param.grad is not None],\n","    buffers=None,\n","    num_data_points=1,\n","    labels=labels.flatten(),\n","    local_hyperparams=None,\n",")"]},{"cell_type":"markdown","id":"10d3f62a","metadata":{"id":"10d3f62a"},"source":["### Reconstruct data from the update"]},{"cell_type":"code","execution_count":null,"id":"91ade4a2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"elapsed":724,"status":"error","timestamp":1719230610499,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"91ade4a2","outputId":"5a2e3fbb-c178-4809-d9f4-1da8e1bc0741"},"outputs":[],"source":["# Attack:\n","reconstructed_user_data, stats = attacker.reconstruct(server_payload, shared_data, secrets, dryrun=False)"]},{"cell_type":"code","execution_count":null,"id":"a768cbdb","metadata":{},"outputs":[],"source":["print"]},{"cell_type":"code","execution_count":null,"id":"6a910a92","metadata":{"executionInfo":{"elapsed":39,"status":"aborted","timestamp":1719226265804,"user":{"displayName":"Omar Elfatairy","userId":"17777768049567417733"},"user_tz":-120},"id":"6a910a92"},"outputs":[],"source":["# Metrics?:\n","from utils.analysis import report\n","true_user_data = {'data': data, 'labels': labels}\n","metrics = report(reconstructed_user_data,\n","    true_user_data,\n","    server_payload,\n","    model, compute_ssim=False) # Can change to true and install a package...\n","print(f\"MSE: {metrics['mse']}, PSNR: {metrics['psnr']}, LPIPS: {metrics['lpips']}, SSIM: {metrics['ssim']} \")"]},{"cell_type":"markdown","id":"1777d351","metadata":{"id":"1777d351"},"source":["### Plot ground-truth data"]},{"cell_type":"code","execution_count":null,"id":"e0484998","metadata":{"id":"e0484998"},"outputs":[],"source":["plot_data(data_cfg_default, true_user_data, setup)\n","\n","# Create the \"images\" folder if it doesn't exist\n","if not os.path.exists(\"images\"):\n","    os.makedirs(\"images\")\n","\n","# Save the images inside the \"images\" folder\n","plt.savefig(\"images/true_user_data.png\")\n"]},{"cell_type":"markdown","id":"f410d7fd","metadata":{"id":"f410d7fd"},"source":["### Now plot reconstructed data"]},{"cell_type":"code","execution_count":null,"id":"2e7dd96c","metadata":{"id":"2e7dd96c"},"outputs":[],"source":["plot_data(data_cfg_default, reconstructed_user_data, setup)\n","# Save the images inside the \"images\" folder\n","plt.savefig(\"images/reconstructed_user_data.png\")"]},{"cell_type":"code","execution_count":null,"id":"0504255f","metadata":{"id":"0504255f"},"outputs":[],"source":["\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}
